{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa0f9488",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T15:17:13.075316Z",
     "start_time": "2025-04-13T15:17:12.689712Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b66f46-e2c3-4aa1-8a5d-09814633fdcd",
   "metadata": {},
   "source": [
    "# A step to step guide to understand attention mechanisms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a5f4066",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T15:17:42.808786Z",
     "start_time": "2025-04-13T15:17:42.798907Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"../latex/figures/MultiheadAttention.svg\" style=\"width: 100%;\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_path = '../latex/figures/MultiheadAttention.svg'\n",
    "display(HTML(f'<img src=\"{image_path}\" style=\"width: 100%;\">'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7194c16e-632a-4f8d-b0d2-a1e593fdcc11",
   "metadata": {},
   "source": [
    "In this notebook, we aim to provide a step-by-step elucidation of the Multihead-Attention mechanism, utilizing generated visualizations and concrete numerical values, as a supplementary elaboration to the explanations presented in the project report. In order to render these explanations reasonably comprehensible, it is imperative that we temporarily depart from the high-dimensional visualization presented above (which mirrors the dimensionalities employed in the 'Attention Is All You Need' paper and maintains an input sequence length of $n=7$, ensuring consistency with the example *'I try to understand the Transformer architecture'* in the report).\n",
    "\n",
    "Let us imagine that we are left with only one sentence of length $n=3$ to elucidate the attention mechanism, namely, *'I love Transformers'*. Furthermore, we will assume a model dimensionality of 4 instead of 512 dimensions, and the calculations will be divided into 2 attention heads. This automatically determines the size or dimensionality of an individual attention head. All of these parameters are set in the following code cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9410aeb0-23c8-45ad-97e5-1c7605df4012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "# Number of attention heads\n",
    "h = 2\n",
    "\n",
    "# Dimensions\n",
    "d_model = 4\n",
    "d_k = d_v = d_model // h\n",
    "\n",
    "# Sequence length\n",
    "n = len(\"I love Transformers\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0354f305-dca8-40e2-a23b-d0862fc90545",
   "metadata": {},
   "source": [
    "As we progress further in this notebook, we will now proceed step by step from left to right through the upper visualization.\n",
    "\n",
    "## Step 1: Initialize input matrices\n",
    "First, we need to initialize the gray-colored encoder input of size $(n, d_{model})$. Since this notebook does not directly build upon the two preceding architectural components of the Transformer - namely, the **input embedding** and **positional encoding** - we will initialize this matrix with arbitrary values ranging from 0 to 9. Specifically, we will designate that the first row of the matrix corresponds to a 4-dimensional representation of the word 'I', the second row to the representation of the word 'love', and the third row to the representation of the word 'Transformers' (we can readily assume this for now, as we have omitted the initial Transformer steps in this context).\n",
    "\n",
    "The three matrices $Q$ (pink), $K$ (orange), and $V$ (yellow) are initially just three copies of this encoder input matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "579d69e9-b3f7-41b7-9fbc-09f1d10e60ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input matrix\n",
    "np.random.seed(0)  # Set seed for reproducibility\n",
    "input_matrix = np.random.randint(10, size=(n, d_model))  # 3x4 matrix with random values between 0 and 9\n",
    "\n",
    "# Copies of the input matrix for Q, K and V\n",
    "Q = input_matrix.copy()\n",
    "K = input_matrix.copy()\n",
    "V = input_matrix.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a92da2",
   "metadata": {},
   "source": [
    "Let's take a look at how our randomly generated representations for the input sequence *'I love Transformers'* appear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1452dc56-8c44-4e1d-892e-11d21d630281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Matrix = Q = K = V: \n",
      "[[5 0 3 3]\n",
      " [7 9 3 5]\n",
      " [2 4 7 6]]\n"
     ]
    }
   ],
   "source": [
    "# Printing\n",
    "print(f\"Input Matrix = Q = K = V: \\n{input_matrix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf99a961-ad21-44aa-bfff-337a379b828e",
   "metadata": {},
   "source": [
    "## Step 2: Initialize parameter matrices\n",
    "As evident from the visualization, matrices $Q$, $K$, and $V$ are subjected to matrix multiplication with their corresponding parameter or projection matrices, denoted as $W_i^Q$, $W_i^K$, and $W_i^V$, respectively. The index 'i' signifies the specific attention head under consideration.\n",
    "\n",
    "But how to initialize the parameter matrices?</br>\n",
    "To ensure more robust training, the initialization of these parameter matrices is a crucial step. In practice, weight matrices are initialized with carefully chosen initial values. In this context, we employ the Xavier Uniform Initialization method, introduced in 2010 in the paper titled \"[Understanding the difficulty of training deep feedforward neural networks](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)\" by Xavier Glorot and Yoshua Bengio. This initialization strategy ensures that the weights are distributed in such a way that the variance of signals is preserved as they propagate through the layers of a network. This preservation of signal variance helps prevent the issues of vanishing or exploding gradients during training, a particularly important consideration in deep architectures such as the Transformer model. It enables more efficient and effective weight adjustments during the learning process.\n",
    "\n",
    "In the following code cell, we initialize the weight matrices $W_i^Q$, $W_i^K$, and $W_i^V$ independently for each attention unit, representing the different heads of the Multihead Attention mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3310530-ae27-44a4-973f-f362decde3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xavier_uniform_init(input_size, output_size) -> np.ndarray:\n",
    "    limit = np.sqrt(6 / (input_size + output_size))\n",
    "    return np.random.uniform(-limit, limit, (input_size, output_size))\n",
    "# xavier uniform initialization of the weight matrices\n",
    "W_Q = np.array([xavier_uniform_init(d_model, d_k) for _ in range(h)])\n",
    "W_K = np.array([xavier_uniform_init(d_model, d_k) for _ in range(h)])\n",
    "W_V = np.array([xavier_uniform_init(d_model, d_v) for _ in range(h)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243b2420-2179-4454-932e-26e008529f7c",
   "metadata": {},
   "source": [
    "As we previously defined two attention heads at the beginning of this notebook, we consequently have two weight matrices each for Queries, Keys, and Values, as illustrated below for $W^Q$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65792ef1-0524-4189-8cb5-f88759547daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.88657405, -0.45468741],\n",
       "        [-0.04466977,  0.62433746],\n",
       "        [-0.04004566, -0.21443041],\n",
       "        [ 0.67215753, -0.32520768]],\n",
       "\n",
       "       [[ 0.29634374, -0.26351692],\n",
       "        [ 0.91431032, -0.71929844],\n",
       "        [ 0.74017452, -0.05278391],\n",
       "        [ 0.6018215 ,  0.04095496]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "410540a7-c8a7-4961-9ed5-2816c1abefa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert W_Q[0].shape == W_Q[1].shape\n",
    "W_Q[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f21641-1390-4969-8d02-5fe2652b2bbf",
   "metadata": {},
   "source": [
    "As we can see, the two generated weight matrices have a dimensionality of $(4, 2)$, which corresponds to the $d_{model}$ rows and $d_v=d_k$ columns. Since we have an additional dimension for the number of attention heads, in the visualization, we observe third-level tensors $W_Q$, $W_K$, and $W_V$. In our specific example, these tensors would have a dimensionality of $(4, 2, 2)$. We can imagine that we stack the individual matrices together along the third dimension.\n",
    "\n",
    "*Note:*</br>\n",
    "As evident from the notation, the distinction between matrices $W_i^Q$ and tensors $W^Q$ is crucial for understanding the architecture of the Multihead Attention mechanism.\n",
    "* The matrix $W_i^Q$ refers to the weight matrix associated with the $i$-th attention head for the query component, represented as a 2D matrix\n",
    "* In constrast, $W^Q$ denotes the entire collection of these weight matrices for all heads, assembled into a 3D tensor. Each \"slice\" of this tensor corresponds to a specific head's weights, $W_i^Q$, thus encapsulating the multi-head aspect of the mechanism within a single tensorial structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9922d90-715c-4ae4-ae30-013ea20e5e51",
   "metadata": {},
   "source": [
    "## Step 3: Calculating the projected matrices $Q_i^\\prime$, $K_i^\\prime$ and $V_i^\\prime$\n",
    "Here we will obtain the projected matrices $Q_i^\\prime$, $K_i^\\prime$ and $V_i^\\prime$ that build the tensors $Q^\\prime$, $K^\\prime$ and $V^\\prime$ shown in the visualization. Let's first take a look at how we get the new query representations for the two attention heads by applying $Q \\times W_i^Q$ for each head:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2978da01-26dd-4200-8496-de85188557c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-2.53653461, -3.89235132],\n",
       "        [-3.36739554,  0.16689562],\n",
       "        [ 1.80079842, -1.86428392]],\n",
       "\n",
       "       [[ 5.50770678, -1.35307145],\n",
       "        [15.53283014, -8.27188133],\n",
       "        [13.0420794 , -3.52798521]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_prime = np.array([np.dot(Q, W) for W in W_Q])\n",
    "Q_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbd94ca-b6c3-405f-a830-0181ada8bd67",
   "metadata": {},
   "source": [
    "This tensor therefore consists of the two matrices $Q_1^\\prime$ und $Q_2^\\prime$ that have the dimensionalitity $(n,d_k)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3c46bd7-ec91-48b8-9e94-7ceed26fc0df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert Q_prime[0].shape == Q_prime[1].shape\n",
    "Q_prime[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a77223-abfa-4235-a7aa-4371cf910311",
   "metadata": {},
   "source": [
    "Similarly we obtain the matrices $K_i^\\prime$ and $V_i^\\prime$ for $i=1,2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cae5f90b-b627-4143-b038-0ab18fe703eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K_1_prime: \n",
      "[[ 3.18209156 -2.04023375]\n",
      " [ 5.26836762 -1.73965563]\n",
      " [ 4.67550023 -8.09978892]]\n",
      "V_1_prime: \n",
      "[[ 0.34381182  1.1983878 ]\n",
      " [ 6.06952461 -6.1297034 ]\n",
      " [ 5.6468306  -2.31171686]]\n",
      "K_2_prime: \n",
      "[[-0.41383514 -2.08497727]\n",
      " [-6.48627938 -4.77384381]\n",
      " [-8.23726599 -1.59698938]]\n",
      "V_2_prime: \n",
      "[[ 4.22924766  4.64235554]\n",
      " [ 2.69400362 -2.69442861]\n",
      " [ 3.31644301  4.78472233]]\n"
     ]
    }
   ],
   "source": [
    "K_prime = np.array([np.dot(K, W) for W in W_K])\n",
    "V_prime = np.array([np.dot(V, W) for W in W_V])\n",
    "for i, (K_i_prime, V_i_prime) in enumerate(zip(K_prime, V_prime)):\n",
    "    print(f\"K_{i+1}_prime: \\n{K_i_prime}\")\n",
    "    print(f\"V_{i+1}_prime: \\n{V_i_prime}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6b1951-9232-434f-9b6d-0630493b7297",
   "metadata": {},
   "source": [
    "Attention should be drawn to two things:</br>\n",
    "* The original matrices $Q$, $K$ and $V$ had dimensions of $(n,d_{model})$, where $d_{model}$ represents the embedding size for each word in the sequence. After projection, the resulting matrices $Q_i^\\prime$, $K_i^\\prime$ and $V_i^\\prime$ have dimensions $(n, d_k)$ or $(n, d_v)$, which are the reduced dimensions for each attention head. For our specific example *\"I love Transformers\"*, this means that the original 4-dimensional vector representations of the words are transformed into two separate 2-dimensional representations for each attention head. This allows different attention heads to capture different aspects of the same word. For instance, one head might focus on the syntactic role of *\"I\"*, whie another might attend to its semantic context. By doing so, the model can parallelly process and integrate diverse information streams, enhancing its ability to discern and utilize complex dependencies within the text.\n",
    "* While the initial matrices $Q$, $K$ and $V$ were identical copies derived from the input encoding, representing the same information for each token, the projection process creates distinct matrices $Q_i^\\prime$, $K_i^\\prime$ and $V_i^\\prime$. This divergence is deliberate. By applying different weight matrices $W_i^Q$, $W_i^K$ and $W_i^V$ for each head, the model generates varied representations for the queries, keys and values.\n",
    "  \n",
    "This means that in the Multihead Attention architecture, diversity emerges on two levels. Firstly, by splitting the original word representations into multiple vectors, we enable different attention heads ($h$) to focus on diverse aspects of the data, introducing the first level of diversity through the multi-head structure itself. Additionally, applying distinct weight matrices ($W_i^Q$, $W_i^K$ and $W_i^V$) to the identical input matrices $Q$, $K$ and $V$ creates a second level of diversity. Each projected matrix $Q_i^\\prime$, $K_i^\\prime$ and $V_i^\\prime$ now encodes different information, even though they originate from the same input data. This allows the model to recognize and respond to a broader spectrum of information within the data, crucial for understanding complex dependencies in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5646a87-0dd0-4c50-9f8b-4555a761c6f8",
   "metadata": {},
   "source": [
    "#### Intermediate stop - next steps\n",
    "In the next steps, the attention function is applied according to the formula $$Attention(Q,K,V)= softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$ as described in \"[Attention Is All You Need](https://arxiv.org/abs/1706.03762)\".</br>\n",
    "Since we are dealing with multi-head attention, we calculate the whole thing for the queries, keys and values of each individual attention head according to $$MultiHead(Q,K,V)=Concat(head_1,...,head_h)W^O$$ where $$head_i=Attention(QW_i^Q,KW_i^K,VW_i^V)$$ We then append the results of the individual attention heads to each other again and project the concatenated matrix once more using the projection matrix $W^O$ to obtain the final attention values. However, we proceed step by step. In practice, this is all done in parallel so that the arithmetic operations are as efficient as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5466d88-5028-4a32-a932-9ba8a64bbe19",
   "metadata": {},
   "source": [
    "## Step 4: Calculating the scaled attention scores\n",
    "In this step, we calculate the attention scores, which determine how much focus to allocate to each word in the input sequence for generating the final output representation. These scores are obtained by taking the **dot product** of the projected query matrix $Q_i^\\prime$ with the transpose of the projected key matrix $K_i^\\prime$ for each attention head. The result of this calculation is represented in the visualization by the red tensor of dimensions $(n, n, h)$. \n",
    "\n",
    "To ensure numerical stability and avoid gradients that are too small during backpropagation, we **scale** the attention scores by dividing by the square root of the key dimensionality, $\\sqrt{d_k}$. This scaling process adjusts the representations of *\"I love Transformers\"*, not just reflecting the individual words but also their contextual relationship within the sequence. Consequently, the model directs its focus more effectively.\n",
    "\n",
    "The raw, scaled attention scores from each head are then passed through a **softmax** function to normalize them into probabilities, represented as a light brown tensor in our visualization. The softmax function ensures that each row of the attention scores — corresponding to the attention distribution from one query word to all key words in a single attention head — sums to 1. This normalization allows the model to assign a probability distribution over all words, quantifying the relative importance each word has on another, thereby enabling the model to attend selectively to the most pertinent information for each word in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18421111-2135-4983-abd9-50c67dc3a8dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[8.32207466e-07, 8.62661112e-09, 9.99999159e-01],\n",
       "        [9.79261626e-01, 7.06124878e-03, 1.36771253e-02],\n",
       "        [5.06701339e-05, 4.85740226e-04, 9.99463590e-01]],\n",
       "\n",
       "       [[9.99999999e-01, 7.02243021e-10, 3.67172510e-14],\n",
       "        [1.00000000e+00, 7.32297340e-23, 2.77013209e-39],\n",
       "        [1.00000000e+00, 3.91078473e-22, 1.37240079e-32]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
    "scaled_attention_scores = np.array([softmax(Q_prime[i].dot(K_prime[i].T) / np.sqrt(d_k)) for i in range(h)])\n",
    "scaled_attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e66e46cd-34ff-4b2f-9946-adf9a66665c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_attention_scores[0].sum(axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102f0ef8-c585-471e-add0-dfab92d7dfc6",
   "metadata": {},
   "source": [
    "*Notice:*</br>\n",
    "Each attention head computes its own set of scaled scores, potentially focusing on different parts of the input sequence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353b175b-580f-455c-9018-96f40a5fa1ba",
   "metadata": {},
   "source": [
    "## Step 5: Calculate the weighted attention scores\n",
    "In the next code cell, we compute the weighted attention scores by multiplying the normalized attention scores from Step 4 with the yellow visualized value matrix $V_i^\\prime$ for each attention head. This operation essentially applies the calculated attention to the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a07cd0db-540d-40ac-b4c2-83abb1b94238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 5.64682619, -2.31171397],\n",
       "        [ 0.45677255,  1.09863418],\n",
       "        [ 5.64676721, -2.31339355]],\n",
       "\n",
       "       [[ 4.22924766,  4.64235554],\n",
       "        [ 4.22924766,  4.64235554],\n",
       "        [ 4.22924766,  4.64235554]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_scores = np.array([scaled_attention_scores[i].dot(V_prime[i]) for i in range(h)])\n",
    "weighted_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b6527d-8423-40c8-a5d8-0e9128700f7b",
   "metadata": {},
   "source": [
    "Each row of the resulting tensor (the dark brown one of dimensionality $(n, d_v, h)$ in the visualization) represents the attention-weighted representations of a word in the sequence, informed by its context. Specifically, the first row reflects how the first word 'attends to' every other word, combining the values from $V_i^\\prime$ weighted by the attention scores. This produces a contextually enriched representation of the first word based on its relevance to the rest of the sequence. In essence, each row in the weighted score matrix for each head is a composite representation that integrates the contextual information as determined by the attention mechanism across all words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4598cef1-5e60-4ebc-aecf-621723ca4a31",
   "metadata": {},
   "source": [
    "## Step 6: Concatenate the weighted attention scores\n",
    "In Step 6 of the Transformer model, we concatenate the weighted attention scores across all attention heads to form a unified representation. This concatenation is performed because each attention head may focus on different aspects of the input sequence, and by combining them, we capture a more comprehensive representation of the sequence.\n",
    "\n",
    "The concatenation is done along the dimension representing the number of heads (usually the last dimension of the tensor). In NumPy, this is achieved using `np.concatenate`, specifying the axis along which the concatenation should occur. Here, axis=1 is used because we are concatenating the second dimension of the tensor, which aligns with the attention heads in a typical implementation where each head's output is a separate matrix in an array. After concatenation, the resulting tensor has information from all heads merged along the specified axis, ready for further processing. Thus, from the third-level tensor with dimensionality $(n, d_v, h)$, as illustrated in the visualization, a matrix with dimensionality $(n, d_v \\cdot h)$ is obtained. Since $d_{model} = d_v \\cdot h$, we consequently obtain a matrix that has the same size as our original gray input matrix, as previously depicted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e69081d1-6358-4309-b68c-020f2d8d05df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.64682619, -2.31171397,  4.22924766,  4.64235554],\n",
       "       [ 0.45677255,  1.09863418,  4.22924766,  4.64235554],\n",
       "       [ 5.64676721, -2.31339355,  4.22924766,  4.64235554]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_values = np.concatenate(weighted_scores, axis=1)\n",
    "concatenated_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85bda7b9-0150-4f1a-aaa2-b27e9c54c0fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7355df-6cb8-4006-86f3-0030b64f2601",
   "metadata": {},
   "source": [
    "## Step 7: Project the final matrix\n",
    "Lastely, the concatenated output from all attention heads is projected through the weight matrix $W^O$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aeaf6256-2aef-4453-9ec0-684abb0505b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.3765234 ,  6.13875477,  3.44813173, -0.03779159],\n",
       "       [ 3.6536207 ,  4.44609421,  5.25015372, -0.89010674],\n",
       "       [ 7.3761135 ,  6.13921767,  3.44763211, -0.03725722]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_O = xavier_uniform_init(h * d_v, d_model)\n",
    "output = concatenated_values.dot(W_O)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8545d23-84b8-4fc5-bafc-193dab1efc72",
   "metadata": {},
   "source": [
    "This projection serves to synthesize the information from all heads, effectively allowing the model to combine the different feature representations captured by each head into a unified output. This step is essential for the Transformer architecture, as it distills the broad array of features into a format suitable for further processing by subsequent layers of the network. The result is an output matrix that maintains the original embedding dimensionality $d_{model}$​ while encapsulating a rich, multi-perspective understanding of the input sequence. This matrix now contains the final attention values!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
