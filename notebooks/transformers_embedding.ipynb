{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66ea7b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import torch "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b04a69",
   "metadata": {},
   "source": [
    "# Transformers\n",
    "Here we use the transformer model [T5](https://huggingface.co/docs/transformers/model_doc/t5) to translate an english sentence into german.\n",
    "## Input Embedding\n",
    "### Tokenization\n",
    "For more information on hugging face tokenizers see [Summary of the tokenizers](https://huggingface.co/docs/transformers/tokenizer_summary).\n",
    "\n",
    "Generally T5 is trained using [SentencePiece](https://huggingface.co/docs/transformers/tokenizer_summary#sentencepiece) in combination with [Unigram](https://huggingface.co/docs/transformers/tokenizer_summary#unigram) (click [here](https://huggingface.co/docs/tokenizers/quicktour) for building an own tokenizer). SentencePiece considers the input as a raw input stream, encompassing spaces in the character set for utilization. It then employs the unigram algorithm to construct the appropriate vocabulary. The decoding process with SentencePiece is quite straightforward, as all tokens can be simply concatenated, and `_` is replaced by a space.  \n",
    "#### Transition words into IDs (and back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f37bcb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sarah/Documents/Transformer/.venv/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:820: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_hidden_states` is. When `return_dict_in_generate` is not `True`, `output_hidden_states` is ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Using t5 model\n",
    "checkpoint = \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(checkpoint, legacy=False)\n",
    "model = T5ForConditionalGeneration.from_pretrained(checkpoint, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aa68ed-d230-4feb-ac2a-1425e3857b2d",
   "metadata": {},
   "source": [
    "Let's start by examining the tokens within the vocabulary, observing the tokens positioned at each index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "537aecb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad>\n",
      "</s>\n",
      "<unk>\n",
      "\n",
      "X\n",
      ".\n",
      "Ich\n",
      "try\n"
     ]
    }
   ],
   "source": [
    "voc_first_indices = [0, 1, 2, 3, 4, 5, 1674, 653]\n",
    "for voc_token in voc_first_indices:\n",
    "    print(tokenizer.decode(voc_token))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e3b99d-3892-4421-89cf-01173d1f70e0",
   "metadata": {},
   "source": [
    "As we can see, the first three positions in the vocabulary are occupied by the special tokens `pad`, `/s` (equivalent to `EOS`), and `unk`. Following these, various individual letters or characters appear, and further down in the vocabulary, complete words are encountered (such as `Ich` at position `1674`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada6b7ff-bc88-4b84-b81a-7c9d3785f430",
   "metadata": {},
   "source": [
    "##### Tokenize the input sentence and show the input IDs\n",
    "Next, we examine how the input sequence *\"I try to understand the Transformer architecture\"* is represented as **IDs** in the computer, so that the Transformer model is able to work with it.\n",
    "\n",
    "*Note:* T5 works well on a variety of tasks out-of-the-box by prepending a different prefix to the input corresponding to each task. For our translation example we will use the prefix *\"translate English to German: \"*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0f90971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[13959,  1566,    12,  2968,    10,    27,   653,    12,   734,     8,\n",
       "         31220,  4648,     1]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_sentence = \"I try to understand the Transformer architecture\"\n",
    "task_prefix = \"translate English to German: \"\n",
    "input_text = task_prefix + english_sentence\n",
    "\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e61ea5-fd19-4ad6-bc62-0c71807c29f0",
   "metadata": {},
   "source": [
    "##### Reconstructing the input sentence from the IDs\n",
    "Similarly, the IDs will contain the index of each of those **tokens** in the tokenizer's vocabulary. So let's convert the IDs to tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "778c944a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁translate',\n",
       " '▁English',\n",
       " '▁to',\n",
       " '▁German',\n",
       " ':',\n",
       " '▁I',\n",
       " '▁try',\n",
       " '▁to',\n",
       " '▁understand',\n",
       " '▁the',\n",
       " '▁Transformer',\n",
       " '▁architecture',\n",
       " '</s>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(input_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3864b22e-0efc-4f68-bd7f-fc9c685864ba",
   "metadata": {},
   "source": [
    "##### Run the model and show the token IDs\n",
    "During the inference phase, it is advisable to employ the `generate()` function. This approach manages the process of encoding the input and conveying the encoded hidden states to the decoder through cross-attention layers. Subsequently, it proceeds to generate the decoder output in an auto-regressive manner. Please see [here](https://huggingface.co/blog/how-to-generate) for more information on how to generate text with Transformers.\n",
    "\n",
    "The decoder output is also represented by the token IDs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cc0db93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,  1674,     3, 27085,    15,     6,    67, 31220,    18, 23533,\n",
       "            23, 15150,  2905,   170, 19163,     1]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = model.generate(input_ids, do_sample=False)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8ce1b6-c6dd-43e2-b2da-cf19508be626",
   "metadata": {},
   "source": [
    "##### Convert output IDs to tokens\n",
    "After converting the token IDs back into tokens, the resulting translation, including the special tokens, is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cecce36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<pad> Ich versuche, die Transformer-Architektur zu verstehen</s>'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_output = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "decoded_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28e28f7-e5d7-4f31-b248-da0c78bc83ad",
   "metadata": {},
   "source": [
    "*Note*: T5 uses the `pad` token ID as the start token ID of the decoder.\n",
    "\n",
    "So far we have seen how to turn each word of the input sentence into an ID that identifies that token (or that word in this example). That's the first step of the Transformer architecture! In the same manner, we have now witnessed the final step of the Transformer - that is, how the token IDs produced by the decoder are converted back into tokens/words. This conversion enables us to ultimately obtain the translation of the input sentence in German, making it readable for us humans."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4629eeac",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "The obtained numbers after tokenization - like `653` representing the word `try` - do not really have meaning encoded into them. And therefore the next step that the model goes through to cature some of the meaning behind the words (and what they could represent) is called **embedding**.\n",
    "#### Breathe meaning into numbers\n",
    "So how can we breathe meaning into numbers? The way that's done is through a so called **embedding matrix**.\n",
    "##### Showing the embedding matrix of the model\n",
    "This matrix is constructed by converting all token IDs of the entire vocabulary (including the IDs associated with special tokens) into high-dimensional vectors (specifically, `512`-dimensional). This results in the formation of an embedding matrix with dimensions $(n\\_vocab, d\\_model)$. These numeric vector representations of 512 numbers capture some of the ideas or the meanings behind each token and that is what the model uses to really make sense of the input text. Therefore creating this embedding matrix is part of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "778fd7df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32128, 512)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_input_embeddings()  # Dimensions are: (Number of tokens in vocabulary, dimension of model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca568d57",
   "metadata": {},
   "source": [
    "In the remainder of this section, an example will illustrate the embedding for the input token `try` with ID `653`.\n",
    "\n",
    "##### Get token with input ID 653 ('try')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2eae7234",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'try'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_ids[0][6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98e4163",
   "metadata": {},
   "source": [
    "##### Get the embedding vector of the token with the input ID 653 ('try')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a1a949b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 16.6250,   0.8086, -24.6250, -19.7500, -17.6250,  -7.6875,   1.0312,\n",
       "         -2.5781,  -0.6602,  22.0000, -49.7500,   4.1562,   0.8008,   5.2188,\n",
       "        -23.6250,  12.0625,  14.6875,  -3.1094,   0.4609,  -8.5625,   1.0000,\n",
       "         -3.1250,  -2.2188, -11.5000,  -6.4375,  24.5000,  13.1250,  -0.4023,\n",
       "          3.1562,  21.3750,  10.0625, -13.5000, -17.0000,  23.0000, -28.6250,\n",
       "         -9.0625,  17.7500, -21.5000, -10.6250,  11.4375,   6.9688,  -6.9688,\n",
       "          8.7500,   2.6406,   0.2236,  -2.5312,   5.0312, -21.2500,  23.5000,\n",
       "         36.2500,   2.2812, -40.7500, -16.7500, -20.8750,  -6.0000,  10.8125,\n",
       "         16.0000,   6.5312,  -6.8750, -18.2500,  -6.1875,   5.9688,  23.1250,\n",
       "        -22.3750,   7.8438,  30.3750,  10.6875,  26.3750,  -3.2812,  -3.7812,\n",
       "         -3.7969, -14.1875, -24.1250, -29.7500, -11.3125,   2.8281,   8.9375,\n",
       "         -9.5625,  -3.5781,  -5.2188,   2.3594,  15.3750,   3.7500,  25.1250,\n",
       "          2.9531,   0.3047,  25.3750, -15.1875, -24.5000,  -6.4375,  -3.2344,\n",
       "          5.7812, -14.5625,   2.6094, -15.6875, -12.5625,  -5.5625,  13.8750,\n",
       "        -15.2500,  11.3750, -16.3750,  15.0625,  -8.9375, -11.8750,  -7.6250,\n",
       "         11.7500,   6.2188, -21.2500,  20.7500,   0.2695, -22.3750,   2.7812,\n",
       "         -1.6484, -40.0000,  10.0625,   4.3438, -41.2500, -29.2500,  55.0000,\n",
       "         13.4375,  13.8125,   5.6562,   9.9375,  -0.7539,  -5.1875,  -4.3750,\n",
       "          4.9062, -25.8750,  14.5000,   4.3438,  22.6250, -26.2500,   7.5625,\n",
       "        -19.1250,   1.7031,  17.7500, -10.5000,  -6.7188, -29.1250, -24.2500,\n",
       "        -25.3750,   1.7578,   8.4375,  49.7500,   8.9375,   8.1250,  -0.3887,\n",
       "        -11.5000, -12.5625,  10.7500,  33.7500, -26.5000, -10.4375, -50.7500,\n",
       "        -15.5000, -13.5625,  22.3750, -21.2500, -11.6250,  12.4375, -29.2500,\n",
       "         -1.0938,  22.0000, -14.8125,  -9.8125,  13.8125,  25.6250, -15.6875,\n",
       "         11.2500,  13.3125,  -1.5234,  40.5000,  14.6875,  -1.1406,   2.5625,\n",
       "        -10.2500, -48.2500,   4.8438, -16.3750,  -1.2891,  22.2500,  16.3750,\n",
       "         -1.9062,  15.1875,  22.1250,  -1.8906, -10.0000,   7.8750,  -0.5469,\n",
       "          1.3594,  11.6875,  27.7500, -24.5000,   5.7812, -22.1250,  -4.1562,\n",
       "         29.5000, -29.2500, -13.7500,   9.3125,  -6.1250,  -2.6094,  21.2500,\n",
       "          9.6250,  26.7500,   3.7344, -16.2500,  -8.2500,  18.6250,  -0.3145,\n",
       "         18.8750, -13.0000,   9.1875,  34.7500,  -7.8750,   1.8594,  -3.5312,\n",
       "         12.9375,   6.4375,   4.3125, -64.0000, -13.0000,  -1.9062, -31.7500,\n",
       "          1.2344,  -5.6250,  28.5000,  15.8750,   2.2031,  -2.3125,  33.5000,\n",
       "         44.5000,   3.8750, -20.5000,   7.6250,  -6.7500,  15.0000,  12.0000,\n",
       "        -28.1250,   6.0312, -22.7500,  -7.0000,  14.9375,  18.6250,   9.8125,\n",
       "         -4.5938,  -5.8125,   1.2266,   8.1875,  -8.6250,   2.9844,  -8.0625,\n",
       "          2.7656,  -6.0312,  10.8750,  -1.6250, -56.7500, -18.2500,  29.1250,\n",
       "         -5.7188,   6.9375,  -7.6562,   5.0312,  -9.8750,   7.9062,  23.8750,\n",
       "        -17.0000, -30.8750, -13.4375, -15.3125,  -0.6172,   5.1250,   0.9102,\n",
       "         -5.3125,  12.4375,   8.5000, -14.0000,  33.0000, -23.1250,  10.6250,\n",
       "         15.8125,  -3.2344,  30.6250,  14.5625, -38.5000,  18.3750, -30.8750,\n",
       "         -7.7500, -10.8750, -21.7500,  -5.3125,  47.2500, -22.5000, -10.3750,\n",
       "          7.5000,  -1.8906,  42.0000, -23.2500,  11.8125, -13.3125,  22.7500,\n",
       "          4.3438,   3.2188,  21.2500,  14.1875, -15.9375,  19.7500,   1.6016,\n",
       "         -9.8750,  -3.0000,  10.4375, -26.0000,   6.9375,  13.5625,  -2.0625,\n",
       "         -2.3594,  11.8750,  32.0000,   3.6562,  -3.8281, -28.0000, -18.6250,\n",
       "         50.5000,   5.0000,   9.1875,   2.5000,  10.0000,  39.5000,  12.8125,\n",
       "         -2.5781,   1.9844,  13.5625,  24.2500, -27.8750,  22.3750,   1.5234,\n",
       "        -10.5000, -12.1875,  18.1250,  26.8750,  -0.4082,   6.9375,  -5.0312,\n",
       "         24.7500,  -2.1719,   4.8750, -20.0000, -13.0000,   4.8750, -22.1250,\n",
       "        -14.6875,  11.0000,   4.6250,  -7.1562,   7.2812,  -1.4297,  11.4375,\n",
       "          5.0938,  16.3750,  17.5000,  12.2500, -27.0000, -12.3750,   1.7031,\n",
       "          5.1875,  11.1250, -23.0000,  -4.1562, -11.0000,   3.9531,  14.6875,\n",
       "          4.8438,  16.6250, -17.8750,  -2.7344, -14.5000, -20.5000,  28.2500,\n",
       "         21.3750,  -0.0933,  65.5000,   8.8125, -18.7500,  -0.6680,  -4.0312,\n",
       "         -7.5625, -17.1250,  11.8750,  23.8750,   2.0781, -11.1250,  -4.7188,\n",
       "         14.2500, -22.7500, -23.3750,  19.6250,   5.6250,   3.2969,  -1.0156,\n",
       "         -3.5469, -17.8750,   3.5625,  11.5625,  -2.6875,   3.3125,  17.8750,\n",
       "        -31.5000,   8.3750,   0.3750,   1.3438,  21.0000,  -1.0625,  -1.2422,\n",
       "          7.8438,  24.0000,  12.0000, -26.3750, -17.3750, -14.0625,   8.8750,\n",
       "         18.7500,  -4.5938, -48.0000, -25.0000,  -4.4062, -16.5000,  32.5000,\n",
       "         11.5000,  -8.5625,  18.2500,  -3.5469,  39.2500,  16.5000,   9.3750,\n",
       "        -19.5000,  -0.8516, -26.3750,  -5.6875,  44.2500,   3.7031,  25.3750,\n",
       "         19.6250, -16.3750,  11.1250,   9.3125,   6.6562,  -8.8125,  10.0625,\n",
       "         32.0000,   6.0312,  31.7500,  -3.8594, -12.8125,  -6.2188,  15.5625,\n",
       "         15.1250,  18.1250,  20.5000,  17.1250,  17.8750, -12.8125,   1.2812,\n",
       "         49.7500,  46.5000,  35.5000,   4.4375, -26.8750,   1.6172,   1.9297,\n",
       "        -13.2500,  60.7500,  -8.8750, -13.7500, -14.1875, -13.5000,   8.6250,\n",
       "         -2.2188,  19.5000, -15.8750, -35.2500,  -8.4375, -11.5000, -26.2500,\n",
       "         47.5000,   0.4043,  31.8750,   1.4688,  12.3125,  -2.8125,  -3.4531,\n",
       "          7.8438, -30.0000, -26.1250,  21.8750, -31.5000,  -0.3906, -20.7500,\n",
       "         20.6250,  35.2500,   4.0312,  29.7500,   5.2812, -18.5000, -24.2500,\n",
       "         -7.6250,  11.0000,   5.1250, -16.1250,  -4.1875,  -6.2188,  31.8750,\n",
       "         13.4375], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.embed_tokens(torch.tensor(653))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7f2af46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The embedding vector is of dimension 512!\n"
     ]
    }
   ],
   "source": [
    "print(f\"The embedding vector is of dimension {len(model.encoder.embed_tokens(torch.tensor(653)))}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2440d57",
   "metadata": {},
   "source": [
    "The example above only shows a single example. You can also do batched inference, like so:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e827bd7",
   "metadata": {},
   "source": [
    "### Extra\n",
    "\n",
    "Padding tokens play a vital role in Transformer implementations when dealing with batched inputs of varying lengths. Batching is employed for computational efficiency and parallel processing during Transformer training. However, the inherent variability in the length of natural language sequences leads to batches containing inputs of unequal sizes, making them unsuitable for direct transformation into fixed-size tensors. To address this challenge, practitioners utilize padding and truncation techniques. Padding involves appending a designated `pad` token to shorter sequences within the batch or adjusting them to conform to the model's maximum accepted sequence length, ensuring uniform sequence lengths across the batch. This uniformity is critical for processing the batch as a single tensor, enabling efficient parallelization and training. Conversely, truncation manages lengthy sequences by shortening them to meet specified length criteria. For advanced strategies and a detailed explanation of the provided API, please refer to the section on [Padding and truncation](https://huggingface.co/docs/transformers/pad_truncation).\n",
    "\n",
    "#### Padding the outputs to the longest sentence when encoding multiple sentences\n",
    "When encoding multiple sentences, you can automatically pad the outputs to the longest sentence present by doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0abe73f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[13959,  1566,    12,  2968,    10,   363,   103,    25,   103,    58,\n",
       "             1,     0,     0,     0],\n",
       "        [13959,  1566,    12,  2968,    10,    27,   653,    12,   734,     8,\n",
       "         31220,  4648,     5,     1]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_1 = \"What do you do?\"\n",
    "input_2 = \"I try to understand the Transformer architecture.\"\n",
    "sentences = [input_1, input_2]\n",
    "inputs = tokenizer([task_prefix + sentence for sentence in sentences], return_tensors=\"pt\", padding=True)\n",
    "inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e070bbf",
   "metadata": {},
   "source": [
    "As expected, the first sentence, which is three words shorter than the second, is extended by three `0`s so that both sentences have the same length.\n",
    "\n",
    "*A brief reminder:* </br>\n",
    "`0` corresponds to the token ID associated with the `PAD` token. This becomes particularly evident after converting the IDs into tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e3e745e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁translate',\n",
       " '▁English',\n",
       " '▁to',\n",
       " '▁German',\n",
       " ':',\n",
       " '▁What',\n",
       " '▁do',\n",
       " '▁you',\n",
       " '▁do',\n",
       " '?',\n",
       " '</s>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9104c42c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁translate',\n",
       " '▁English',\n",
       " '▁to',\n",
       " '▁German',\n",
       " ':',\n",
       " '▁I',\n",
       " '▁try',\n",
       " '▁to',\n",
       " '▁understand',\n",
       " '▁the',\n",
       " '▁Transformer',\n",
       " '▁architecture',\n",
       " '.',\n",
       " '</s>']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e9a88c",
   "metadata": {},
   "source": [
    "To obtain the decoder output in a human-readable token form, you can use the `batch_decode` method provided by HuggingFace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "177f1098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Was tun Sie?', 'Ich versuche, die Transformer-Architektur zu verstehen.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sequences = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    do_sample=False,  # disable sampling to test if batching affects output\n",
    ")\n",
    "tokenizer.batch_decode(output_sequences, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dd736a",
   "metadata": {},
   "source": [
    "And now we repeat this step without hiding the special tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "810f65d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad> Was tun Sie?</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>',\n",
       " '<pad> Ich versuche, die Transformer-Architektur zu verstehen.</s>']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(output_sequences, skip_special_tokens=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
