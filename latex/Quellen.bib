@INPROCEEDINGS{Isolation_Forest_1,
  author={Liu, Fei Tony and Ting, Kai Ming and Zhou, Zhi-Hua},
  booktitle={2008 Eighth IEEE International Conference on Data Mining}, 
  title={Isolation Forest}, 
  year={2008},
  volume={},
  number={},
  pages={413-422},
  doi={10.1109/ICDM.2008.17}
}

@article{Isolation_Forest_2,
author = {Liu, Fei Tony and Ting, Kai Ming and Zhou, Zhi-Hua},
title = {Isolation-Based Anomaly Detection},
year = {2012},
issue_date = {March 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {1},
issn = {1556-4681},
url = {https://doi.org/10.1145/2133360.2133363},
doi = {10.1145/2133360.2133363},
abstract = {Anomalies are data points that are few and different. As a result of these properties, we show that, anomalies are susceptible to a mechanism called isolation. This article proposes a method called Isolation Forest (iForest), which detects anomalies purely based on the concept of isolation without employing any distance or density measure---fundamentally different from all existing methods.As a result, iForest is able to exploit subsampling (i) to achieve a low linear time-complexity and a small memory-requirement and (ii) to deal with the effects of swamping and masking effectively. Our empirical evaluation shows that iForest outperforms ORCA, one-class SVM, LOF and Random Forests in terms of AUC, processing time, and it is robust against masking and swamping effects. iForest also works well in high dimensional problems containing a large number of irrelevant attributes, and when anomalies are not available in training sample.},
journal = {ACM Trans. Knowl. Discov. Data},
month = {mar},
articleno = {3},
numpages = {39},
keywords = {outlier detection, Anomaly detection, ensemble methods, binary tree, isolation, isolation forest, random tree ensemble}
}

@article{Random_Forest,
author = {Breiman, L},
year = {2001},
month = {10},
pages = {5-32},
title = {Random Forests},
volume = {45},
journal = {Machine Learning},
doi = {10.1023/A:1010950718922}
}

@article{Isolation_Forest_Extended,
	doi = {10.1109/tkde.2019.2947676},
	url = {https://doi.org/10.1109%2Ftkde.2019.2947676},
	year = 2021,
	month = {apr},
	publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
	volume = {33},
	number = {4},
	pages = {1479--1489},
	author = {Sahand Hariri and Matias Carrasco Kind and Robert J. Brunner},
	title = {Extended Isolation Forest},
	journal = {{IEEE} Transactions on Knowledge and Data Engineering}
}

@article{Isolation_Forest_Generalized,
	title = {Generalized isolation forest for anomaly detection},
	journal = {Pattern Recognition Letters},
	volume = {149},
	pages = {109-119},
	year = {2021},
	issn = {0167-8655},
	doi = {https://doi.org/10.1016/j.patrec.2021.05.022},
	url = {https://www.sciencedirect.com/science/article/pii/S0167865521002063},
	author = {Julien Lesouple and Cédric Baudoin and Marc Spigai and Jean-Yves Tourneret},
	keywords = {Anomaly detection, Isolation forest},
	abstract = {This letter introduces a generalization of Isolation Forest (IF) based on the existing Extended IF (EIF). EIF has shown some interest compared to IF being for instance more robust to some artefacts. However, some information can be lost when computing the EIF trees since the sampled threshold might lead to empty branches. This letter introduces a generalized isolation forest algorithm called Generalized IF (GIF) to overcome these issues. GIF is faster than EIF with a similar performance, as shown in several simulation results associated with reference databases used for anomaly detection.}
}

@misc{Isolation_Forest_Deep,
	doi = {10.48550/ARXIV.2206.06602},	
	url = {https://arxiv.org/abs/2206.06602},	
	author = {Xu, Hongzuo and Pang, Guansong and Wang, Yijie and Wang, Yongjun},	
	keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},	
	title = {Deep Isolation Forest for Anomaly Detection},	
	publisher = {arXiv},	
	year = {2022},	
	copyright = {Creative Commons Attribution 4.0 International}
}

@INPROCEEDINGS{Anomaly_Detection_Isolation_Forest_Time_Series_Data_Power_consumption,
  author={Mao, Wei and Cao, Xiu and zhou, Qinhua and Yan, Tong and Zhang, Yongkang},
  booktitle={2018 International Conference on Power System Technology (POWERCON)}, 
  title={Anomaly Detection for Power Consumption Data based on Isolated Forest}, 
  year={2018},
  volume={},
  number={},
  pages={4169-4174},
  doi={10.1109/POWERCON.2018.8602251}
  }
  
  @INPROCEEDINGS{Anomaly_Detection_Isolation_Forest_Time_Series_Data_Network,
  author={Chun-Hui, Xiao and Chen, Su and Cong-Xiao, Bao and Xing, Li},
  booktitle={2018 4th Annual International Conference on Network and Information Systems for Computers (ICNISC)}, 
  title={Anomaly Detection in Network Management System Based on Isolation Forest}, 
  year={2018},
  volume={},
  number={},
  pages={56-60},
  doi={10.1109/ICNISC.2018.00019}
  }

@ARTICLE{Deep_Learning_AD_Time_Series_Data,
  author={Choi, Kukjin and Yi, Jihun and Park, Changhwa and Yoon, Sungroh},
  journal={IEEE Access}, 
  title={Deep Learning for Anomaly Detection in Time-Series Data: Review, Analysis, and Guidelines}, 
  year={2021},
  volume={9},
  number={},
  pages={120043-120065},
  doi={10.1109/ACCESS.2021.3107975}
  }
  
@article{Anomaly_Detection_Survey_classical,
author = {Chandola, Varun and Banerjee, Arindam and Kumar, Vipin},
year = {2009},
month = {07},
pages = {},
title = {Anomaly Detection: A Survey},
volume = {41},
journal = {ACM Comput. Surv.},
doi = {10.1145/1541880.1541882}
}

@misc{Formal_Algorithms_for_Transformers_DeepMind,
      title={Formal Algorithms for Transformers}, 
      author={Mary Phuong and Marcus Hutter},
      year={2022},
      eprint={2207.09238},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{TranAD,
      title={TranAD: Deep Transformer Networks for Anomaly Detection in Multivariate Time Series Data}, 
      author={Shreshth Tuli and Giuliano Casale and Nicholas R. Jennings},
      year={2022},
      eprint={2201.07284},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{Attention_is_all_you_need,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Mathematical_view_of_attention_models_in_deep_learning,
  title={A mathematical view of attention models in deep learning},
  author={Ji, Shuiwang and Xie, Yaochen and Gao, Hongyang}
}

@misc{BERT,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{GPT,
  added-at = {2020-07-14T16:37:42.000+0200},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  biburl = {https://www.bibsonomy.org/bibtex/273ced32c0d4588eb95b6986dc2c8147c/jonaskaiser},
  interhash = {5c343ed9a31ac52fd17a898f72af228f},
  intrahash = {73ced32c0d4588eb95b6986dc2c8147c},
  keywords = {final thema:transformer},
  timestamp = {2020-07-14T16:49:42.000+0200},
  title = {Improving language understanding by generative pre-training},
  year = 2018
}

@misc{Neural_machine_translation_by_jointly_learning_to_align_and_translate,
      title={Neural Machine Translation by Jointly Learning to Align and Translate}, 
      author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
      year={2016},
      eprint={1409.0473},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{Structured_attention_networks,
      title={Structured Attention Networks}, 
      author={Yoon Kim and Carl Denton and Luong Hoang and Alexander M. Rush},
      year={2017},
      eprint={1702.00887},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{Residual_Connections,
      title={Deep Residual Learning for Image Recognition}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1512.03385},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@inproceedings{Transformer_Implementation_Harvard,
  author    = {Guillaume Klein and
               Yoon Kim and
               Yuntian Deng and
               Jean Senellart and
               Alexander M. Rush},
  title     = {OpenNMT: Open-Source Toolkit for Neural Machine Translation},
  booktitle = {Proc. ACL},
  year      = {2017},
  url       = {https://doi.org/10.18653/v1/P17-4012},
  doi       = {10.18653/v1/P17-4012}
}
@article{Transformer_Implementation_Google_Tensor2Tensor,
  author    = {Ashish Vaswani and Samy Bengio and Eugene Brevdo and
    Francois Chollet and Aidan N. Gomez and Stephan Gouws and Llion Jones and
    \L{}ukasz Kaiser and Nal Kalchbrenner and Niki Parmar and Ryan Sepassi and
    Noam Shazeer and Jakob Uszkoreit},
  title     = {Tensor2Tensor for Neural Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1803.07416},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.07416},
}

@misc{Transformers_HuggingFace_Natural_Language,
      title={HuggingFace's Transformers: State-of-the-art Natural Language Processing}, 
      author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush},
      year={2020},
      eprint={1910.03771},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{Conv_Seq2Seq_Learning,
      title={Convolutional Sequence to Sequence Learning}, 
      author={Jonas Gehring and Michael Auli and David Grangier and Denis Yarats and Yann N. Dauphin},
      year={2017},
      eprint={1705.03122},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@book{Goodfellow_Handbuch,
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  title={Deep Learning},
  subtitle={das umfassende Handbuch : Grundlagen, aktuelle Verfahren und Algorithmen, neue Forschungsans{\"a}tze},
  edition={1. Auflage},
  publisher={mitp},
  address={Frechen},
  year={2018},
  pages={xxii, 883 Seiten},
  language={ger},
  isbn={978-3-95845-700-3 and 3-95845-700-2},
  series={mitp Business},
  note={Literaturverzeichnis: Seite 813-870},
  keywords={Deep learning ; Deep learning ; Deep learning / Maschinelles Lernen},
  library={UB [Signatur: LN-U 10-18939]},
}
@misc{PositionalEncodingTransformers,
      title={The Impact of Positional Encoding on Length Generalization in Transformers}, 
      author={Amirhossein Kazemnejad and Inkit Padhi and Karthikeyan Natesan Ramamurthy and Payel Das and Siva Reddy},
      year={2023},
      eprint={2305.19466},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{VisionTransformer,
      title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}, 
      author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
      year={2021},
      eprint={2010.11929},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{AudioTransformers,
      title={Audio Transformers:Transformer Architectures For Large Scale Audio Understanding. Adieu Convolutions}, 
      author={Prateek Verma and Jonathan Berger},
      year={2021},
      eprint={2105.00335},
      archivePrefix={arXiv},
      primaryClass={cs.SD}
}

@misc{RoboticsTransformer,
      title={RT-1: Robotics Transformer for Real-World Control at Scale}, 
      author={Anthony Brohan and Noah Brown and Justice Carbajal and Yevgen Chebotar and Joseph Dabis and Chelsea Finn and Keerthana Gopalakrishnan and Karol Hausman and Alex Herzog and Jasmine Hsu and Julian Ibarz and Brian Ichter and Alex Irpan and Tomas Jackson and Sally Jesmonth and Nikhil J Joshi and Ryan Julian and Dmitry Kalashnikov and Yuheng Kuang and Isabel Leal and Kuang-Huei Lee and Sergey Levine and Yao Lu and Utsav Malla and Deeksha Manjunath and Igor Mordatch and Ofir Nachum and Carolina Parada and Jodilyn Peralta and Emily Perez and Karl Pertsch and Jornell Quiambao and Kanishka Rao and Michael Ryoo and Grecia Salazar and Pannag Sanketi and Kevin Sayed and Jaspiar Singh and Sumedh Sontakke and Austin Stone and Clayton Tan and Huong Tran and Vincent Vanhoucke and Steve Vega and Quan Vuong and Fei Xia and Ted Xiao and Peng Xu and Sichun Xu and Tianhe Yu and Brianna Zitkovich},
      year={2023},
      eprint={2212.06817},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}

@article{X-Transformer,
author = {Liu, Huey-Ing and Chen, Wei-Lin},
year = {2022},
month = {04},
pages = {4502},
title = {X-Transformer: A Machine Translation Model Enhanced by the Self-Attention Mechanism},
volume = {12},
journal = {Applied Sciences},
doi = {10.3390/app12094502}
}

@inproceedings{TransformerDiffEncoderDecoderLayers,
author = {Xu, Hongfei and Genabith, Josef and Liu, Qiuhui and Xiong, Deyi},
year = {2021},
month = {01},
pages = {74-85},
title = {Probing Word Translations in the Transformer and Trading Decoder for Encoder Layers},
doi = {10.18653/v1/2021.naacl-main.7}
}


@inproceedings{WNLI,
title = "The winograd schema challenge",
abstract = "In this paper, we present an alternative to the Turing Test that has some conceptual and practical advantages. A Wino-grad schema is a pair of sentences that differ only in one or two words and that contain a referential ambiguity that is resolved in opposite directions in the two sentences. We have compiled a collection of Winograd schemas, designed so that the correct answer is obvious to the human reader, but cannot easily be found using selectional restrictions or statistical techniques over text corpora. A contestant in the Winograd Schema Challenge is presented with a collection of one sentence from each pair, and required to achieve human-level accuracy in choosing the correct disambiguation.",
author = "Levesque, {Hector J.} and Ernest Davis and Leora Morgenstern",
year = "2012",
language = "English (US)",
isbn = "9781577355601",
series = "Proceedings of the International Conference on Knowledge Representation and Reasoning",
publisher = "Institute of Electrical and Electronics Engineers Inc.",
pages = "552--561",
booktitle = "13th International Conference on the Principles of Knowledge Representation and Reasoning, KR 2012",
note = "13th International Conference on the Principles of Knowledge Representation and Reasoning, KR 2012 ; Conference date: 10-06-2012 Through 14-06-2012",

}

@misc{TransformerFeedForward,
      title={Transformer Feed-Forward Layers Are Key-Value Memories}, 
      author={Mor Geva and Roei Schuster and Jonathan Berant and Omer Levy},
      year={2021},
      eprint={2012.14913},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{GoogleNeuralMachineTranslation,
      title={Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation}, 
      author={Yonghui Wu and Mike Schuster and Zhifeng Chen and Quoc V. Le and Mohammad Norouzi and Wolfgang Macherey and Maxim Krikun and Yuan Cao and Qin Gao and Klaus Macherey and Jeff Klingner and Apurva Shah and Melvin Johnson and Xiaobing Liu and Łukasz Kaiser and Stephan Gouws and Yoshikiyo Kato and Taku Kudo and Hideto Kazawa and Keith Stevens and George Kurian and Nishant Patil and Wei Wang and Cliff Young and Jason Smith and Jason Riesa and Alex Rudnick and Oriol Vinyals and Greg Corrado and Macduff Hughes and Jeffrey Dean},
      year={2016},
      eprint={1609.08144},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{MitigatingNNOverconfidenceWithLogitNormalization,
  title = {Mitigating Neural Network Overconfidence with Logit Normalization},
  author = {Hongxin Wei and Renchunzi Xie and Hao Cheng and Lei Feng and Bo An and Yixuan Li},
  booktitle = {Proceedings of the 39th International Conference on Machine Learning},
  pages = {23631-23644},
  year = {2022},
  editor = {Kamalika Chaudhuri and Stefanie Jegelka and Le Song and Csaba Szepesvari and Gang Niu and Sivan Sabato},
  volume = {162},
  series = {Proceedings of Machine Learning Research},
  publisher = {PMLR},
  url = {https://proceedings.mlr.press/v162/wei22d.html},
  pdf = {https://proceedings.mlr.press/v162/wei22d/wei22d.pdf},
}

@misc{AnomalyBERT,
      title={AnomalyBERT: Self-Supervised Transformer for Time Series Anomaly Detection using Data Degradation Scheme}, 
      author={Yungi Jeong and Eunseok Yang and Jung Hyun Ryu and Imseong Park and Myungjoo Kang},
      year={2023},
      eprint={2305.04468},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{AnomalyTransformer,
      title={Anomaly Transformer: Time Series Anomaly Detection with Association Discrepancy}, 
      author={Jiehui Xu and Haixu Wu and Jianmin Wang and Mingsheng Long},
      year={2022},
      eprint={2110.02642},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@Article{TGAN-AD,
AUTHOR = {Xu, Liyan and Xu, Kang and Qin, Yinchuan and Li, Yixuan and Huang, Xingting and Lin, Zhicheng and Ye, Ning and Ji, Xuechun},
TITLE = {TGAN-AD: Transformer-Based GAN for Anomaly Detection of Time Series Data},
JOURNAL = {Applied Sciences},
VOLUME = {12},
YEAR = {2022},
NUMBER = {16},
ARTICLE-NUMBER = {8085},
URL = {https://www.mdpi.com/2076-3417/12/16/8085},
ISSN = {2076-3417},
ABSTRACT = {Anomaly detection on time series data has been successfully used in power grid operation and maintenance, flow detection, fault diagnosis, and other applications. However, anomalies in time series often lack strict definitions and labels, and existing methods often suffer from the need for rigid hypotheses, the inability to handle high-dimensional data, and highly time-consuming calculation costs. Generative Adversarial Networks (GANs) can learn the distribution pattern of normal data, detecting anomalies by comparing the reconstructed normal data with the original data. However, it is difficult for GANs to extract contextual information from time series data. In this paper, we propose a new method, Transformer-based GAN for Anomaly Detection of Time Series Data (TGAN-AD), The transformer-based generators of TGAN-AD can extract contextual features of time series data to prompt the performance. TGAN-AD&rsquo;s discriminator can also assist in determining abnormal data. Anomaly scores are calculated through both the generator and the discriminator. We have conducted comprehensive experiments on three public datasets. Experimental results show that our TGAN-AD has better performance in anomaly detection than the state-of-the-art anomaly detection techniques, with the highest Recall and F1 values on all datasets. Our experiments also demonstrate the high efficiency of the model and the optimal choice of hyperparameters.},
DOI = {10.3390/app12168085}
}

@article{AD_Transformer_GraphStructures,
  title={Learning graph structures with transformer for multivariate time-series anomaly detection in IoT},
  author={Chen, Zekai and Chen, Dingshuo and Zhang, Xiao and Yuan, Zixuan and Cheng, Xiuzhen},
  journal={IEEE Internet of Things Journal},
  volume={9},
  number={12},
  pages={9179--9189},
  year={2021},
  publisher={IEEE}
}

@article{VariationalTransformerAD,
  title={Variational transformer-based anomaly detection approach for multivariate time series},
  author={Wang, Xixuan and Pi, Dechang and Zhang, Xiangyan and Liu, Hao and Guo, Chang},
  journal={Measurement},
  volume={191},
  pages={110791},
  year={2022},
  publisher={Elsevier}
}

@article{StackedTransformersAD,
  title={Time-series anomaly detection with stacked Transformer representations and 1D convolutional network},
  author={Kim, Jina and Kang, Hyeongwon and Kang, Pilsung},
  journal={Engineering Applications of Artificial Intelligence},
  volume={120},
  pages={105964},
  year={2023},
  publisher={Elsevier}
}

@article{TiSAT,
  title={TiSAT: time series anomaly transformer},
  author={Doshi, Keval and Abudalou, Shatha and Yilmaz, Yasin},
  journal={arXiv preprint arXiv:2203.05167},
  year={2022}
}

@article{AD_AdversarialTransformer,
  title={Multivariate time series anomaly detection with adversarial transformer architecture in the Internet of Things},
  author={Zeng, Fanyu and Chen, Mengdong and Qian, Cheng and Wang, Yanyang and Zhou, Yijun and Tang, Wenzhong},
  journal={Future Generation Computer Systems},
  volume={144},
  pages={244--255},
  year={2023},
  publisher={Elsevier}
}

@inproceedings{MAML,
  title={Model-agnostic meta-learning for fast adaptation of deep networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={1126--1135},
  year={2017},
  organization={PMLR}
}

@article{witkowski2017internet,
  title={Internet of things, big data, industry 4.0--innovative solutions in logistics and supply chains management},
  author={Witkowski, Krzysztof},
  journal={Procedia engineering},
  volume={182},
  pages={763--769},
  year={2017},
  publisher={Elsevier}
}

@article{tuli2020modelling,
  title={Modelling for prediction of the spread and severity of COVID-19 and its association with socioeconomic factors and virus types},
  author={Tuli, Shreshth and Tuli, Shikhar and Verma, Ruchi and Tuli, Rakesh},
  journal={MedRxiv},
  pages={2020--06},
  year={2020},
  publisher={Cold Spring Harbor Laboratory Press}
}

@article{DeepLearningAD,
  title={Deep learning for anomaly detection: A survey},
  author={Chalapathy, Raghavendra and Chawla, Sanjay},
  journal={arXiv preprint arXiv:1901.03407},
  year={2019}
}

@article{POT,
  title={Anomaly Detection in Streams with Extreme Value Theory},
  author={Alban Siffer and Pierre-Alain Fouque and Alexandre Termier and Christine Largou{\"e}t},
  journal={Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  year={2017},
  url={https://api.semanticscholar.org/CorpusID:22057}
}



